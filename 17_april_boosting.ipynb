{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9147c5b-be76-4f47-bf1a-23fecaadc5f1",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25508a54-e4f3-42e3-bed7-7a4a1f4ccd81",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d346a34e-fade-4881-8be1-c28516bb9b37",
   "metadata": {},
   "source": [
    "Gradient Boosting ia a powerful boosting algorithm that combines several weak learners into strong learners , in which each new mdoel is trained to minimize the loss function such as mean squared error or cross-entropy of the previous model using gradient descent.\n",
    "\n",
    "Gradient boost is one of the most powerful techniques for building predictive models for both classification and Regression problems.\n",
    "\n",
    "Inn Gradient Boosting for regression , an estimator builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions.\n",
    "In each stage, a regression tree is fit on the negative gradient of the given loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1dcfdc-4f80-4c25-be33-9c2786f94d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee0111-4088-4684-9051-026c78e62469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89bd15d2-7f91-428d-aa12-a506dc115e1e",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe880c-3d75-4cfe-8f55-e5c221234950",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008bfa6-409d-471e-814f-311087ed9ece",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to simple models that do only slightly better than random chance. Decision trees are used as the weak learner in gradient boosting, specifically regression trees that output real values for splits and whose output can be added together, allowing subsequent models outputs to be added and “correct” the residuals in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae019e-df06-44bd-8e46-5a1257153c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d03db-2928-4c08-a01c-3e966d07966b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43ccfe73-54a1-4782-94e8-5014f996e1f9",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd58f366-a466-434a-ab2f-f362ef9928a1",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bc3c61-4f79-4fb6-93a2-469b36227229",
   "metadata": {},
   "source": [
    "The intuition behind Gradient Boosting is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better. Once we reach a stage where residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting).\n",
    "\n",
    "The algorithm relies on the intuition that the best possible next model, when combined with the previous models, minimizes the overall prediction errors. The key idea is to set the target outcomes from the previous models to the next model in order to minimize the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4623291e-cbd1-4081-95c1-8634f53f6c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffec32-8db4-42b2-92da-ae449653c322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74737034-4fef-4e22-848c-3125bd8555ee",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806cb06-ecb3-4d8d-8bd3-20dd514b7d7c",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79419f-9726-470b-b8cf-b4d03364ee50",
   "metadata": {},
   "source": [
    "Gradient Boosting algorithm builds an ensemble of weak learners by repetitively leveraging the patterns in residuals and strengthening a model with weak predictions and making it better. Once we reach a stage where residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting). The algorithm relies on the intuition that the best possible next model, when combined with the previous models, minimizes the overall prediction errors. The key idea is to set the target outcomes from the previous models to the next model in order to minimize the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aee8894-dee6-4ef1-aa04-3918f682c820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831d390f-ba5b-4b1d-b3eb-e505d7d9ae44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "544933b7-20f0-408a-9cde-c1fe8747c984",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting \n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd5491-a495-4d7b-9b62-41caf66952fd",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf71f49-7bda-4171-b12f-4753a2b43a0f",
   "metadata": {},
   "source": [
    "The mathematical intuition behind Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "1. Initialize the model with a constant value.\n",
    "2. Calculate the negative gradient of the loss function with respect to the model output.\n",
    "3. Fit a weak learner to the negative gradient.\n",
    "4. Update the model by adding the weak learner’s predictions multiplied by a learning rate.\n",
    "5. Repeat steps 2-4 until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcfe1d5-cfde-4539-88fa-43470752069b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
