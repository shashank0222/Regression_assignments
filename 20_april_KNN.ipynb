{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79f9e91-9418-4cc6-b2c1-dd24415d15ec",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe242a4-7bca-4411-a20e-90435953bcd7",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501cd6e4-be34-4ecf-b555-fec96e8d66b8",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbor (KNN) algorithm is a simple machine learning algorithm based on supervised  learning teechnique. It assumes the similarity between the new data and available cases and puts the new case into the category which is most similar to the available categories.\n",
    "\n",
    "The algorithm stores all the available data and classifies a new data point based on the similarity.KNN algorithm can be used for Regression as well ass for Classification but mostly it is used for Classification problems.\n",
    "\n",
    "It is also called lazy learner algorithm because it does not learn from the training dataset immediately instead it stores the dataset and at the time of classification , it performs an action on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd17e914-d001-4808-badc-94ca32b61902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b1739-95eb-4fc6-9ea4-bca5e74bd79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9778b966-5495-405c-b341-30fe2f8907cd",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e39bef-4f50-43bf-b9be-096ca484e745",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd61d0d-0600-4ce4-b5fe-ea378b31bb50",
   "metadata": {},
   "source": [
    "The value of K in KNN is a hyperparameter that controls the number of neighbors that are considered when classifying a new data point. Choosing the right value of K is very critical to the accuracy of the model. A small value of K means that noise will have higher influence on the result , while a larger value makes it computationally expensive and defeats the basic philosophy behind KNN (points that are near might have similar densities or classes).\n",
    "\n",
    "There is no particular way of choosing the value of K, but here are some common conventions to keep in mind : \n",
    "* choosing a very low value will most likely lead to inaccurate predictions.\n",
    "* The commonly used value of k is 5.\n",
    "* Always use an odd number as the value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6de7a62-08fd-42af-8238-9d78c633daa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b24d31c-7ad7-4cc6-8855-bb491e6a0e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "307ba657-a918-4950-bdb5-07e780cee885",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524501e-ec80-4024-bdc6-0f0b89c265ca",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071bfad-bc2e-409b-9657-fe45e9c9f8b8",
   "metadata": {},
   "source": [
    "The key difference between KNN Classifier and KNN Regressor is that KNN Classifier is used for Classification problem and KNN Regressor is used for Regression problem.\n",
    "\n",
    "KNN classifier tries to predict the output by computing the local probability whereas KNN regressor tries to predict the output by using local average ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c5f45a-fec4-436b-9a8c-83167e96e7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4662e1-5eb5-4a4a-8911-afb16083f6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a736b2b-6b1f-4c93-9a50-28a7edc69575",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e24f0-c240-442b-bf56-9913fe38a066",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f3dda-3725-4691-a5cb-2ff2d6db4cc4",
   "metadata": {},
   "source": [
    "The performance of KNN can be measured using various metrics such as accuracy, precision, recall, F1 score, and confusion matrix. The choice of metric depends on the problem at hand. For example, accuracy is a good metric when the classes are balanced, while precision and recall are better when the classes are imbalanced. The confusion matrix is a table that summarizes the performance of a classification algorithm. It shows the number of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN) for each class ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0a0e7-ec05-4620-8829-8e6f195e170c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578d7702-62f1-4965-940c-e29bf404ae4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e81f22a0-2c19-4a8c-81d8-1ee140a12572",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7657ce9-37f7-419e-9d6e-e71e1a4502aa",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c942e61-c428-4390-8240-ca1e16202406",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the fact that as the number of dimensions increases, the amount of data required to generalize accurately grows exponentially. In other words, as the number of dimensions increases, the data becomes more sparse and the distance between points becomes less meaningful. This can lead to overfitting and poor performance of KNN algorithm. To overcome this problem, we can use dimensionality reduction techniques such as PCA (Principal Component Analysis) or LDA (Linear Discriminant Analysis) to reduce the number of dimensions while preserving most of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538658c-26d3-4b9e-91f8-a2fae44a9916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1d8203-58dd-4bb1-b12d-804d65397d03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "483d7fbc-3a6b-43c2-add4-0af623578b88",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb479b8-247c-49c4-a0fa-59593fc9c268",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9d17c2-da11-4e68-be4a-25889cbacf35",
   "metadata": {},
   "source": [
    "KNN algorithm does not handle missing values well. One way to handle missing values is to impute them with the mean or median of the feature. Another way is to use a distance metric that can handle missing values such as Manhattan distance or Euclidean distance with missing values treated as zero. A third way is to use algorithms that can handle missing values such as decision trees or random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665d05bf-9a7a-4bd8-aee6-6869b711449d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef1f37-f094-41d4-924b-9479bf5940bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b22a245-e705-4f3c-99dc-e1ca342686db",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cef0c7-03c9-4c65-8d97-38359da6a8dd",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a9673-73ab-410d-a221-ce9389bc88f0",
   "metadata": {},
   "source": [
    "KNN classifier is used for classification problems and KNN regressor is used for solving regression problems. The performance of both algorithms depends on the problem at hand. For example, if the response variable is a factor with levels, we use KNN classifier to classify the response into levels. On the other hand, if the response variable is continuous, we use KNN regressor to predict its value. The choice of algorithm depends on the nature of the problem and the type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046efef-975c-4e2e-a853-3fa52e3f9d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad5de4-2d02-4191-8c18-7da422ad6828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "647195db-fb97-40a7-8aa2-0d922ff1917c",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54777a-c594-42b8-8d4f-3f4b5b2761a6",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958345b5-62f1-4277-8d82-7d3aed542ba8",
   "metadata": {},
   "source": [
    "The strengths of the kNN algorithm are : \n",
    "* It is simple to understand and implement.\n",
    "* It can be used for both Classification and Regression problems.\n",
    "* It doest not make any assumptions about the underlying data distribution.\n",
    "\n",
    "The weakness of KNN algorithm are :\n",
    "* The higher value of k can make it computationally expensive.\n",
    "* It is sensitive to the irrelevant features.\n",
    "* It does not work well with the high-dimensional data.\n",
    "\n",
    "These weakness can be addressed by using :\n",
    "* PCA or LDA to reduce the dimensions while preserving most of the information\n",
    "* Feature Selection to select only relevant features.\n",
    "* Distance metrics such as Euclidean distance and Manhattan distance to handle missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0885e854-6880-47c1-af1b-1fdaaa102ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b5c50e-6420-4c52-a40a-af40d936d94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae1e2115-3c50-4a80-8aa5-136125658b00",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f7b81-a45e-4045-8dcd-6549660a6a66",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b588a4aa-ee12-4f75-9c3b-d4f2cfd868d5",
   "metadata": {},
   "source": [
    "The Euclidean distance is the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squared differences between the coordinates of two points. The Manhattan distance is the sum of the absolute differences between the coordinates of two points. It is called Manhattan distance because it is similar to how you would navigate through Manhattan city blocks.\n",
    "\n",
    "In KNN, both Euclidean distance and Manhattan distance can be used as distance metrics to calculate the distance between two points. The choice of metric depends on the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071edd2-e683-4609-b38e-c4938b5a5bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c565e7bb-6693-445c-bbd9-a6299e71bef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed192c68-e6cd-4848-b586-41017ce83c5d",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec86cd8-b93d-41f4-a4b8-6566bb9f05c3",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9add2-199c-4e03-b28d-0e43bc5bf0f7",
   "metadata": {},
   "source": [
    "Feature scaling is crucial for the KNN algorithm as it helps in preventing features with larger magnitudes from dominating the distance calculations. Scaling all features to a common scale gives each feature an equal weight in distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbbf6fd-966b-4594-ade6-3fc6dcfc8ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
