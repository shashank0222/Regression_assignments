{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4a539a-a690-4665-9ba1-e15b02ca02a1",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874507b-ab52-4dff-a739-f5ca19f1e682",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8b95e-0637-46f2-9233-84c6875b7512",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure that represents the goodness of fit of a regression model. It is also called the coefficient of determination. The value of R-square lies between 0 to 1. Where we get R-square equals 1 when the model perfectly fits the data and there is no difference between the predicted value and actual value. However, we get R-square equals 0 when the model does not predict any variability in the model and it does not learn any relationship between the dependent and independent variables.\n",
    "\n",
    "R-squared measures how well the regression line fits the data points. It is calculated by using the following formula:\n",
    "\n",
    "R-squared = 1 - (SSres / SStot)\n",
    "\n",
    "Where SSres is the residual sum of squares and SStot is the total sum of squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5600d825-0912-4cdf-993a-80c00a12ae98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88f356-c784-4249-8539-0307101f0f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ced2fd2d-9363-42ca-86c9-34165bcb570b",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43f6460-d0f9-4ac8-9bfd-46fe1208577e",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6840000-e90b-4dec-be5e-cb6c403e7c38",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that adjusts the variable predictors in regression models. Adjusted R-squared considers and tests different independent variables against the stock index and penalizes you for adding features that are not useful for predicting the target. Adjusted R-squared can be more accurate and can correlate the variables more efficiently than R-squared. However, R-squared cannot be used to determine whether the coefficient estimates and predictions are biased or which regression variable is more important than the other.\n",
    "\n",
    "The formula for adjusted R-squared is:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "Where n is the number of observations and k is the number of independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956c372-0e0f-4449-b32f-b2cb593de34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d61980-ee7e-42d2-8e9d-ecb1168d8ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d35b8419-ae42-45db-893d-aa9cd1c3d75a",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039855e2-bc19-463d-ae08-ee700ef2fcf6",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c574a428-ac06-472a-aa8a-d70012263363",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate when comparing models that have different numbers of independent variables or when overfitting is a concern. Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in a regression model. It penalizes models that have too many variables, making it more accurate in cases where overfitting is a concern.\n",
    "\n",
    "In summary, R-squared is a useful metric when comparing models that have similar numbers of independent variables, while adjusted R-squared is more appropriate when comparing models that have different numbers of independent variables or when overfitting is a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b9f7a-4e5a-49fc-a47c-16c4c16d77a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7dfdc6-e217-4437-912d-66ce492f9186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f6932bb-8119-4952-b622-48f4709a6965",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f137b6-ddc3-434a-8e86-aa4d46d1a34b",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf45aa33-b2d9-45dd-8fed-708b37b8c1e7",
   "metadata": {},
   "source": [
    "RMSE (Root Mean Squared Error), MSE (Mean Squared Error), and MAE (Mean Absolute Error) are metrics used to evaluate the performance of regression models.\n",
    "\n",
    "RMSE is the square root of the average of squared differences between predicted and actual values. It measures the standard deviation of residuals. The formula for RMSE is:\n",
    "\n",
    "RMSE = sqrt(1/n * sum((y_pred - y_true)^2))\n",
    "\n",
    "Where y_pred is the predicted value, y_true is the actual value, and n is the number of observations.\n",
    "\n",
    "MSE is the average of squared differences between predicted and actual values. It measures the average squared difference between the estimated values and true values. The formula for MSE is:\n",
    "\n",
    "MSE = 1/n * sum((y_pred - y_true)^2)\n",
    "\n",
    "Where y_pred is the predicted value, y_true is the actual value, and n is the number of observations.\n",
    "\n",
    "MAE is the average of absolute differences between predicted and actual values. It measures the average absolute difference between the estimated values and true values. The formula for MAE is:\n",
    "\n",
    "MAE = 1/n * sum(|y_pred - y_true|)\n",
    "\n",
    "Where y_pred is the predicted value, y_true is the actual value, and n is the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24f6cd-5865-44bf-b685-803967d5ff19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1158b2-7c78-41ef-9c7d-d84c4067c8dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c25f9a6-0002-4e34-ad6d-f3577b096e20",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b744cc-30b4-46ee-9482-91f0c9ad2af6",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e071c-898e-4c65-a830-c935243cdb71",
   "metadata": {},
   "source": [
    "RMSE, MSE, and MAE are widely used evaluation metrics in regression analysis. Each metric has its own advantages and disadvantages.\n",
    "\n",
    "RMSE is sensitive to outliers and penalizes large errors more than small errors. It is useful when large errors are particularly undesirable.\n",
    "\n",
    "MSE is similar to RMSE but does not take the square root. It is also sensitive to outliers and penalizes large errors more than small errors. It is useful when large errors are particularly undesirable. However, it can be difficult to interpret because it is not on the same scale as the original data.\n",
    "\n",
    "MAE is less sensitive to outliers than RMSE and MSE. It does not penalize large errors more than small errors. It is useful when all errors are equally important. However, it can be difficult to interpret because it is not on the same scale as the original data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cc1d6-21f6-4929-9d09-b644c3c17f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f5bc7-8bde-4c5d-8263-4d66a4f6775f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91e1d840-1e6a-426d-a3fa-b70ce4ab73f1",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715dd4f-8ece-4188-ae62-dcba1f6fae0c",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8b0186-2f15-49e9-95fe-6c664cff9460",
   "metadata": {},
   "source": [
    "Lasso and Ridge regularization are two methods of reducing the complexity and improving the performance of linear regression models by introducing a penalty factor. Lasso regression, also known as L1 regularization, shrinks the coefficients to zero by taking the magnitude of the coefficients. Ridge regression, also known as L2 regularization, shrinks the coefficients to small values by taking the square of the coefficients. Lasso is better for feature selection and ridge is better for handling multicollinearity.\n",
    "\n",
    "Lasso regularization is more appropriate when we have a large number of features and we want to select only a few important features. It is also useful when we want to avoid overfitting by reducing the number of features in our model. Ridge regularization is more appropriate when we have multicollinearity in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2d125-5636-4b9c-a63e-4b3afc712071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bbd967-c12f-4578-89b2-a792109fbb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "397a9838-add3-462b-85db-af3b7b3c8fd2",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e25e69-8572-4e81-a8f7-b712531d51ed",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5478b8-6690-4233-92c8-8bc26874d080",
   "metadata": {},
   "source": [
    "Regularized linear models help to prevent overfitting in machine learning by adding a penalty term to the cost function of the model. This penalty term is used to shrink the coefficients towards zero, which reduces the complexity of the model and helps to avoid overfitting. Regularization techniques such as Lasso and Ridge regression are used to add this penalty term to the cost function.\n",
    "\n",
    "For example, suppose we have a dataset with many features and we want to fit a linear regression model. If we use all the features in our model, we may end up with an overfitted model that performs poorly on new data. To avoid this, we can use Lasso or Ridge regression to add a penalty term to the cost function of our model. This will shrink the coefficients towards zero and reduce the complexity of our model, which will help us avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6410acf-e280-4617-9afc-89462ad05ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bd554e-e80c-451f-a307-6a79e6270d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e6a49c3-842c-4e45-a770-55b8651fed52",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93334943-01f7-4ba9-bfd0-ec05e034d0bd",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3ffa0-5d71-494a-8716-2f74db605940",
   "metadata": {},
   "source": [
    "Regularized linear models have some limitations. One of the main limitations is that they assume linearity in the data. If the data is not linear, then a linear model may not be the best choice for regression analysis.\n",
    "\n",
    "Another limitation is that regularized linear models can be sensitive to the choice of regularization parameter. If the regularization parameter is too small, then the model may overfit the data. If the regularization parameter is too large, then the model may underfit the data.\n",
    "\n",
    "In addition, regularized linear models can be computationally expensive to train, especially when there are many features in the data.\n",
    "\n",
    "Despite these limitations, regularized linear models are still widely used in regression analysis because they can help to prevent overfitting and improve the performance of linear regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed718adf-a3c0-4af7-9ee0-9987c7f989f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab183cda-7702-43b2-966f-08dfbe708508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37e3c05a-c627-4e5a-ad88-b04fad15458c",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b9dea-fd13-4af5-aea1-991d0cf9fd92",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a997e79-3c41-418d-a7f2-18bd60970a68",
   "metadata": {},
   "source": [
    "Both RMSE (Root Mean Squared Error) and MAE (Mean Absolute Error) are evaluation metrics used to measure the performance of regression models. RMSE measures the average distance between the predicted values and the actual values, while MAE measures the average absolute difference between the predicted values and the actual values.\n",
    "\n",
    "In this case, Model B has a lower MAE of 8, which means that it has a smaller average absolute difference between the predicted values and the actual values. This suggests that Model B is a better performer than Model A.\n",
    "\n",
    "However, it’s important to note that both RMSE and MAE have their limitations as evaluation metrics. For example, RMSE is more sensitive to outliers than MAE because it squares the errors. This means that if there are outliers in the data, then RMSE may not be the best choice for evaluating model performance.\n",
    "\n",
    "In addition, both RMSE and MAE do not take into account the direction of the errors. This means that if a model is consistently overestimating or underestimating the actual values, then RMSE and MAE may not capture this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195bf0c2-734b-4e99-a40c-acf3fb2278b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c44924-8486-4e7b-a1a2-7a63beb16299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5fd3d-3c74-44f0-ad21-6fb3018485a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f83cde7-471c-4061-a7aa-90fc0632eaa8",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8acbb70-9577-4f60-824b-3077a5ff43eb",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729aa8af-fae5-47e6-bbfa-02b70a8d33ea",
   "metadata": {},
   "source": [
    "Both Ridge and Lasso regularization are used to reduce the complexity of linear regression models and prevent overfitting. Ridge regularization shrinks the coefficients towards zero by taking the square of the coefficients, while Lasso regularization shrinks the coefficients towards zero by taking the absolute value of the coefficients.\n",
    "\n",
    "In this case, it’s difficult to say which model is better without more information about the data and the specific problem being solved. However, in general, Lasso regularization is better for feature selection because it can shrink some coefficients to exactly zero. This means that Lasso can be used to identify which features are most important for predicting the target variable.\n",
    "\n",
    "On the other hand, Ridge regularization is better for handling multicollinearity because it shrinks all the coefficients towards zero by a small amount. This means that Ridge can be used to reduce the impact of highly correlated features on the model.\n",
    "\n",
    "There are trade-offs and limitations to both types of regularization. For example, Lasso regularization can be sensitive to outliers in the data because it takes the absolute value of the coefficients. In addition, Lasso can only select at most n features if there are n observations in the data.\n",
    "\n",
    "Ridge regularization can also have limitations. For example, if there are many features in the data, then Ridge may not be able to effectively reduce the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae53b9-2832-493d-9848-62d3778417a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
