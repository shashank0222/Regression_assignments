{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb22756-13f0-4d2d-946c-1e065a5a62f3",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8502e9-baee-4fd5-ac5c-b04fb171ff5c",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48491b72-b084-47ed-b02c-efc4fc4ecf67",
   "metadata": {},
   "source": [
    "Ensemble methods are a family of techniques that combine multiple models to improve their overall performance. There are three main classes of ensemble learning methods: bagging , and boosting. The main principle of ensemble methods is to combine weak and strong learners to form strong and versatile learners. Ensemble methods can be used for both classification and regression problems. Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26da954-29d7-41c3-ba00-fa492e0341c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52662d7-17ee-4218-8cba-cd2f4b10a33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54f49426-64d4-491e-b39a-6045fdb252f2",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a24c1-08ec-4ade-9b21-c974c46dab34",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924720d0-2182-435c-b576-15d4033c2ff9",
   "metadata": {},
   "source": [
    "Ensemble methods are used in machine learning to improve the accuracy of models by combining multiple models instead of using a single model. The combined models increase the accuracy of the results significantly.\n",
    "The mechanism for improved performance with ensembles is often the reduction in the variance component of prediction errors made by the contributing models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f06ad8-72a5-4721-9bfb-f95c1d10119b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1af181-7e95-42e2-a537-cad3b44bd797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a58afb-6eb2-4efb-85e3-31784de95d64",
   "metadata": {},
   "source": [
    "Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ad9022-16ab-4402-8fc3-fc8bb0b6a70c",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431228f-568a-49ca-860b-d780b3ed040d",
   "metadata": {},
   "source": [
    "Bagging, also known as Bootstrap aggregating, is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model. Bagging is an ensemble created from decision trees fit on different samples of a dataset. The main two components of bagging technique are the random sampling with replacement (bootstraping) and the set of homogeneous machine learning algorithms (ensemble learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3611db5-b976-409f-a525-9309f08a8de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59debad5-6044-47f3-93ab-1a1389e9a997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92673ffb-98a7-4c4f-b102-12c07c2787f5",
   "metadata": {},
   "source": [
    "Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221ea3a-91fc-476c-b680-ae689c8049ee",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1980e0-a11b-4e59-8a3a-69f3f58f4325",
   "metadata": {},
   "source": [
    "Boosting is an ensemble learning method that combines a set of weak learners into strong learners to minimize training errors. In boosting, a random sample of data is selected, fitted with a model, and then trained sequentially. That is, each model tries to compensate for the weaknesses of its predecessor. Boosting is an iterative technique that adjusts the weight of an observation based on the last classification. If an observation was classified incorrectly, it tries to increase the weight of this observation and vice versa. Boosting is used in classification and regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e90c6f-cec3-49e2-a64a-283e527a19f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f46b3-7c85-460a-b405-58764b9a4cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd906ee3-4c3e-45c2-91e8-6b2fbd1b7fa5",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6457b2e-ad9d-4810-8857-c2e72bf3281f",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbc8995-86d5-4b93-9116-0f9c5c8951e7",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for two main reasons:\n",
    "\n",
    "1. Performance: An ensemble can make better predictions and achieve better performance than any single contributing model.\n",
    "2. Robustness: An ensemble reduces the spread or dispersion of the predictions and model performance. The underlying concept behind ensemble learning is to mitigate the errors or biases that may exist in individual models by leveraging the collective intelligence of multiple models, which ultimately leads to a more precise prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9522e-a543-4f21-a8fb-0c0c6c831f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e65f1b-d20a-4847-8f29-5db28de4b54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "096743b3-90a3-4124-a2b3-76157800b0d3",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f645bc3-39dc-44cc-8a7f-6745dae9a085",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d1f7e-98b3-4dd1-b5c0-f7192b7a4af9",
   "metadata": {},
   "source": [
    "Ensemble techniques are not always better than individual models. There is no absolute guarantee that an ensemble model performs better than an individual model. However, if you build many of those and your individual classifier is weak, your overall performance should be better than an individual model. In machine learning, training multiple models generally outperforms training a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0665ee-6d70-46bd-ab89-7bbf76003422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677495d-5d3e-4744-8ade-b286efdad1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc8ac446-70fd-4ee3-9b2c-6b6777188776",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9920bc-7685-4455-a195-f35a7afcd8ce",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11480ac8-5b74-48fb-9edd-a8bfe13e3823",
   "metadata": {},
   "source": [
    "The bootstrap method is a statistical technique that involves resampling a dataset with replacement to estimate a statistic on the population. The confidence interval is calculated by ordering the bootstrap estimates and taking the endpoints of the interval that covers the desired percentage of the estimates. The bootstrap confidence interval can be used for statistics that have no closed-form or complicated solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0974cb6-83e6-4ddf-b24d-989bcf75034f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66235c0-6df7-4f9d-8227-204706251e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a56c78a6-608f-4a79-875c-85d0b1933af8",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7badcc3-d487-4c9c-a83c-7caf2fdfb51d",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933b44a-97a6-42be-97fd-20f6a0f71543",
   "metadata": {},
   "source": [
    "The bootstrap method is a statistical technique that involves resampling a dataset with replacement to estimate a statistic on the population. The steps involved in bootstrap are:\n",
    "\n",
    "* Randomly select samples from the original dataset with replacement.\n",
    "* Calculate the statistic of interest for each sample.\n",
    "* Repeat steps 1 and 2 many times (usually 1000 or more).\n",
    "* Calculate the mean and standard deviation of the statistics calculated in step 2.\n",
    "* Use the mean and standard deviation to calculate the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1907fbc6-8885-4f5b-a946-ec85b37c1d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6a575-be19-4e56-80b9-23506e4d9259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ba4174f-fd98-4471-ad21-abae2d319c56",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c889f36-0560-4964-a8f4-8326ca493e3c",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6bbf6c-2ecd-4eb1-80b3-b9b0fd957b83",
   "metadata": {},
   "source": [
    "To estimate the 95% confidence interval for the population mean height using bootstrap, we can follow these steps:\n",
    "\n",
    "1. Resample the original sample of 50 trees with replacement to create a new sample of 50 trees.\n",
    "2. Calculate the mean height of the new sample.\n",
    "3. Repeat steps 1 and 2 many times (usually 1000 or more).\n",
    "4. Calculate the mean and standard deviation of the means calculated in step 2.\n",
    "5. Use the mean and standard deviation to calculate the confidence interval.\n",
    "Using this method, we can estimate that the 95% confidence interval for the population mean height is between 14.4 meters and 15.6 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fbb6ce-aca7-478e-8b55-10632acb01e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
