{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c315afa-20f3-4799-b003-0cb4cda89c97",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c270f1-2892-4520-92ee-3adc89291e4d",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52761f4b-2c6e-4547-af59-92c56bc2b21e",
   "metadata": {},
   "source": [
    "The main difference between Euclidean distance metric and Manhattan distance metric is that Euclidean distance is the shortest distance between two points in a straight line while the Manhattan distance is the sum of absolute differences between points across all dimensions.\n",
    "\n",
    "Euclidean distance is more sensitive to outliers while Manhattan distance is less sensitive to outliers and are more robust to noise.\n",
    "\n",
    "The choice of distance metric can affect the performance of a KNN classifier or reegressor. If the data has many irrelevant features or noise , then Manhattan distance metric can improve the performance of a KNN classifier or regressor. On the other hand, if the data has fer irrelevant features, then using a Euclidean distance metric can improve the performance of a KNN classifier or regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2bb1c0-28fc-4d0f-8fd6-972e548ddf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e5d81c-21bf-4268-984c-0c4e4b2b89d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a91fd7e0-ada9-4e53-8dd8-e92195402516",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d63ade-34a4-437e-a929-bc088244636c",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a4486-918a-46da-b893-2d8d5eb1146b",
   "metadata": {},
   "source": [
    "One way to determine the optimal k value is by using cross-validation. In cross-validation, the dataset is divided into training and testing sets. The model is trained on the training set and tested on the testing set for different values of k. The value of k that gives the best performance on the testing set is chosen as the optimal k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3383d9-12bf-4efe-bf13-71255a516f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a9117-0a73-4a79-a3e9-cf183bb956e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f74fa638-7e24-4e76-91c3-925b801a7f8c",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f98047-4f79-4b9d-891b-6febd946778e",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d02b745-1baf-47ec-a604-7af3e24ef76f",
   "metadata": {},
   "source": [
    "The choice of distance metric can affect the performance of a KNN classifier or reegressor. If the data has many irrelevant features or noise , then Manhattan distance metric can improve the performance of a KNN classifier or regressor. On the other hand, if the data has fer irrelevant features, then using a Euclidean distance metric can improve the performance of a KNN classifier or regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837875d8-88ed-4185-867f-4fe99f8ee43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e73bb2-6be7-433d-bf8e-2dca34bee656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6df5b128-665c-45ce-853d-2a684ccef24f",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a266f4-55ff-4c81-8562-8900d5a515f3",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a6f35-6dbe-47f3-b76d-b9c3783fbd38",
   "metadata": {},
   "source": [
    "Some common hyperparameters in KNN classifiers and regressors are:\n",
    "\n",
    "1. The number of neighbors (k) to consider when making predictions.\n",
    "2. The distance metric used to calculate the distance between data points.\n",
    "3. The weight function used to combine the distances of the k nearest neighbors.\n",
    "\n",
    "The weight function used to combine the distances of the k nearest neighbors can also affect the performance of the model. The two most commonly used weight functions are uniform weights and distance weights. Uniform weights give equal weight to all k nearest neighbors while distance weights give more weight to closer neighbors.\n",
    "\n",
    "To tune these hyperparameters, we can use cross-validation. In cross-validation, we divide the dataset into training and testing sets. We train the model on the training set and test it on the testing set for different values of hyperparameters. The value of hyperparameters that gives the best performance on the testing set is chosen as optimal hyperparameters .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390641b-3b7a-4fd2-8e41-7b04e0cbb095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be6b872-02f3-4073-8153-937907961385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac4cb8b4-bcc4-4530-9408-b80e64ca9c10",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b730f2-01e8-42a1-9cae-2edb7f8fa09f",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b1b7f-23d2-40d2-844a-227a546f46a9",
   "metadata": {},
   "source": [
    "The size of the training set affects the performance of a KNN classifier or regressor significantly. If the training set is too small, then the model may overfit to the training data. If the training set is too large, then the model may underfit to the training data. The optimal size of the training set depends on the dataset and how it generalizes to the unseen data.\n",
    "\n",
    "To optimize the size of the training set, we can use cross-validation.  In cross-validation, we divide the dataset into training and testing sets. We train the model on different sizes of training sets and test it on the testing set for different sizes of training sets. The size of the training set that gives the best performance on the testing set is chosen as optimal size of training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2c5fa0-0f7b-4581-93e6-ac82b8959822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773ebe51-9e14-4576-88da-a5c051e2ea99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a995b43-49a0-41a3-9463-3e5143ad51c2",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386a0e3-7956-4074-b952-a9ab60267374",
   "metadata": {},
   "source": [
    "Solution :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5cc9e5-16e8-42c2-83c1-0de06880ab35",
   "metadata": {},
   "source": [
    "Some potential drawbacks of using KNN as a classifier or regressor are:\n",
    "\n",
    "* KNN is computationally expensive for large datasets.\n",
    "* KNN is sensitive to the choice of distance metric.\n",
    "* KNN is sensitive to the choice of hyperparameters such as the number of neighbors (k) and the weight function used to combine the distances of the k nearest neighbors.\n",
    "* KNN does not work well with high-dimensional data.\n",
    "\n",
    "To overcome these drawbacks and improve the performance of the model, we can use techniques such as:\n",
    "\n",
    "* Dimensionality reduction techniques such as PCA to reduce the dimensionality of the data.\n",
    "* Feature selection techniques to select only relevant features.\n",
    "* Cross-validation to tune hyperparameters such as k and weight function.\n",
    "* Approximate nearest neighbor algorithms such as locality-sensitive hashing (LSH) to speed up the computation of nearest neighbors ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40108170-5c9d-4456-a875-1c7097799fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
